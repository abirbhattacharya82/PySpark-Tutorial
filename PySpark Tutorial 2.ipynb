{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceed67f5-af56-41d9-b2b8-b838b5cee8b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n  Using cached pyspark-3.5.1-py2.py3-none-any.whl\r\nCollecting py4j==0.10.9.7\r\n  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\r\nInstalling collected packages: py4j, pyspark\r\nSuccessfully installed py4j-0.10.9.7 pyspark-3.5.1\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-9d1ea9fc-90e1-4ed9-8190-2e0384004d2a/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7863a7b-7169-42aa-9f9f-aba8cd64f11d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=5318080910828743#setting/sparkui/0325-023444-ci9w0i7b/driver-5995427879443567378\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=5318080910828743#setting/sparkui/0325-023444-ci9w0i7b/driver-5995427879443567378\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"demo\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1214c7f-302f-477d-9636-32bc2e113a48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n|     Name|Age|Experience|\n+---------+---+----------+\n|    Krish| 31|        10|\n|Sudhanshu| 30|         8|\n|    Sunny| 29|         4|\n+---------+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('/FileStore/tables/PySparkTutoria/sample1.csv', inferSchema=True, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b46ec96-83f7-477b-bad4-9d1cc067fbe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|     Name|\n+---------+\n|    Krish|\n|Sudhanshu|\n|    Sunny|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# To view a particular column\n",
    "df.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945b1a2f-0c78-4a55-8333-3546c52ac342",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|     Name|Experience|\n+---------+----------+\n|    Krish|        10|\n|Sudhanshu|         8|\n|    Sunny|         4|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Multiple Elements\n",
    "df.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a260439-6d5c-4214-9664-af0f0a9ac827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------------------------+\n|     Name|Age|Experience|Experience after 2 years|\n+---------+---+----------+------------------------+\n|    Krish| 31|        10|                      12|\n|Sudhanshu| 30|         8|                      10|\n|    Sunny| 29|         4|                       6|\n+---------+---+----------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Adding a new Column. For this scenario we will be creating a new column which will be adding 2 years to the current experience and name the column as Experience after 2 years.\n",
    "df.withColumn('Experience after 2 years', df['Experience']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2255bdad-66b6-4595-b435-b01f5b7a1521",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n|     Name|Age|Experience|\n+---------+---+----------+\n|    Krish| 31|        10|\n|Sudhanshu| 30|         8|\n|    Sunny| 29|         4|\n+---------+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Dropping the columns\n",
    "df.drop('Experience after 2 years').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4750bc9f-06bc-4abf-9837-dc54834c63e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n|First Name|Age|Experience|\n+----------+---+----------+\n|     Krish| 31|        10|\n| Sudhanshu| 30|         8|\n|     Sunny| 29|         4|\n+----------+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns\n",
    "df.withColumnRenamed('Name','First Name').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Tutorial 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
