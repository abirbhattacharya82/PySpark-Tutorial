{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0386a4a-375b-4dc8-800a-f9fdc1fb1ae3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /local_disk0/.ephemeral_nfs/envs/pythonEnv-3c77d121-ce11-48bc-bcc6-84bcaa893839/lib/python3.9/site-packages (3.5.1)\r\nRequirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-3c77d121-ce11-48bc-bcc6-84bcaa893839/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-3c77d121-ce11-48bc-bcc6-84bcaa893839/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"tutorials\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a297b2c-b1fc-485f-8e93-bb87ba19f8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|   Subham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"/FileStore/tables/PySparkTutoria/sample3.csv\", inferSchema=True, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533e08dd-103b-44d7-b232-8f9bbc019bee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n|     Name|Age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|   Subham| 23|         2| 18000|\n+---------+---+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Remove records containing all the null values\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6470adf2-9aac-4a13-b160-27b0d46a45c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|   Subham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Parameters of na.drop()\n",
    "# 1) how. the value of how can be either any or all. any suggest that if there are any null values in the row then it will be dropped. Whereas all suggest that if all the values in a certain row is null then it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65f2c9b-43af-46e6-8579-4f08214696f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|   Subham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c93b9d-a81d-4b15-aa3e-37b0cc4b377d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n|     Name|Age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|   Subham| 23|         2| 18000|\n+---------+---+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5efcf26-c092-405e-aebe-b6aaee8910ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) thresh. thresh takes an integer as its value. It suggest the number of non null values to be present in a row to prevent it from getting dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7213ba-d003-4cf3-8810-89620708a986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|   Subham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"any\",thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1314c2e-c7e0-4fbe-a421-2f89a1f86f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) subset. Subset is used when we check for null values in a column. For example the following example will be deleting all the rows where the Experience is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0c3c24-f84e-4f3c-a9fd-ec6f366c5063",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n|     Name|Age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|   Subham| 23|         2| 18000|\n|     null| 34|        10| 38000|\n+---------+---+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=[\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789dcc98-f43e-4f00-a4e2-33d07d12ae07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filling null values with something else\n",
    "# There are 2 parameters here. \n",
    "# 1) value: this is mandatory as it replaces null with that value.\n",
    "# 2) subset: this is again similar to na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885a1828-6f67-46ca-9fdb-69407610d8af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|   Subham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|  Missing|  34|        10| 38000|\n|  Missing|  36|      null|  null|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"Missing\",\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb57f7a-5b9d-4a89-a986-19c8b8966332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Handling missing values of a specific columns based on the mean or median of that specific column. We need an inputer function for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ec1aa5-acaf-41f0-96df-099d2b8c7f32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+------------------+\n|     Name| Age|Experience|Salary|Experience_Imputed|\n+---------+----+----------+------+------------------+\n|    Krish|  31|        10| 30000|                10|\n|Sudhanshu|  30|         8| 25000|                 8|\n|    Sunny|  29|         4| 20000|                 4|\n|     Paul|  24|         3| 20000|                 3|\n|   Harsha|  21|         1| 15000|                 1|\n|   Subham|  23|         2| 18000|                 2|\n|   Mahesh|null|      null| 40000|                 5|\n|     null|  34|        10| 38000|                10|\n|     null|  36|      null|  null|                 5|\n+---------+----+----------+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Replacing the missing values in Experience with the mean of the Experience column\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer=Imputer(\n",
    "    inputCols=['Experience'],\n",
    "    outputCols=['Experience_Imputed']\n",
    ").setStrategy('mean')\n",
    "imputer.fit(df).transform(df).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Tutorial 3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
