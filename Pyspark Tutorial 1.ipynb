{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d396128-76ca-4dcf-af90-913df11d0079",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d37a881a-e308-4998-b9e5-c838af105739",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "###To Run PySpark Commands Here are a few things we have to do:\n",
    "- Installing pyspark\n",
    "```\n",
    "!pip install pyspark\n",
    "```\n",
    "- importing pyspark\n",
    "```\n",
    "import pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ad9330-a38f-4831-85ef-ea21c2364ddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7bdd790-4f1e-490b-bc6d-9dad623b076f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Creating a spark session\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"<app_name>\").getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0fbc0f-25a5-41b2-9b3c-cb5fdc16b3ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"PySparkTutorial\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3085f766-059d-48cb-9c86-fa24ada14a48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading Different Types of Data in PySpark\n",
    "### Here we often use two options\n",
    "1) Headers= To take the topmost row of the dataset as the header of the dataframe\n",
    "2) inferSchema= To take the integer values of the dataset as integer otherwise by default it is string for all. Simillarly for date it will take date and so on.\n",
    "### In the following codeblocks we will be reading the different types of data as Spark Dataframes\n",
    "- CSV\n",
    "- JSON\n",
    "- TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdb14c0-a5cc-47ce-ae36-add19a3d5153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfCSV=spark.read.csv('/FileStore/tables/PySparkTutoria/sample.csv', inferSchema=True, header=True)\n",
    "dfCSV.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7dbc69-026c-4585-9709-050d09bd2100",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfJSON=spark.read.json('/FileStore/tables/PySparkTutoria/sample.json')\n",
    "dfJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50cdfc1e-30c9-4db2-8962-6ad508df9edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfOthers=spark.read.load(\"/FileStore/tables/PySparkTutoria/sample.txt\", format=\"csv\", header=\"true\", inferSchema=\"true\")\n",
    "dfOthers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef6bfde-6693-4a61-ae83-8f487f8c3297",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## To Know the Dataframe\n",
    "### The dataset we deal with can be very huge so we can do certain steps to know our dataframe\n",
    "- describe\n",
    "- statistical data\n",
    "- printSchema\n",
    "- head(num)\n",
    "- take(num)\n",
    "- tail(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e64da8-0d18-4de3-9f75-4e57506580f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfOthers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOthers.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55aaa557-5d6d-405a-8a66-2a451d48d80a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfOthers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b67ce17-3cc0-495b-99b1-86dfea092f82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfOthers.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea71743-59e6-4cd7-ae89-69efaa4f82d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfOthers.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6719230-f73a-448c-8330-f9d4c88c0df7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfOthers.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dfOthers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOthers.columns"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Tutorial 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
